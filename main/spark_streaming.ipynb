{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "notebook_path = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(notebook_path, '..'))\n",
    "sys.path.append(project_root)\n",
    "from _constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Standard library imports\n",
    "from time import sleep\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "from pyspark.sql.types import BinaryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/08 14:40:39 WARN Utils: Your hostname, cthi-Inspiron-5515 resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp2s0)\n",
      "24/07/08 14:40:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/cthi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cthi/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0139d790-c688-4e75-843e-0f389a974142;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.7.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-6 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      ":: resolution report :: resolve 604ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-6 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.7.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.7.0] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   3   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0139d790-c688-4e75-843e-0f389a974142\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/26ms)\n",
      "24/07/08 14:40:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>UAV Detection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x768bfd49c220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.7.0'\n",
    "]\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"UAV Detection\") \\\n",
    ".master(\"local\") \\\n",
    ".config(\"spark.executor.memory\", \"16g\") \\\n",
    ".config(\"spark.driver.memory\", \"16g\") \\\n",
    ".config(\"spark.python.worker.reuse\", \"true\") \\\n",
    ".config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    ".config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"5\") \\\n",
    ".config(\"spark.scheduler.mode\", \"FAIR\")  \\\n",
    ".config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "conf=SparkConf()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP ðŸ’¡ Replace 'model=../params/pt_yolov5n.pt' with new 'model=../params/pt_yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yolo = YOLO(\"../params/pt_yolov5n.pt\")\n",
    "# yolo = YOLO(\"../params/pt_yolov8s.pt\") \n",
    "# yolo = YOLO(\"../params/pt_yolov8n.pt\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast model\n",
    "sc = SparkContext.getOrCreate()\n",
    "broadcast_model = sc.broadcast(yolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Spark's User-Defined Function (UDF) and get broadcasted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = broadcast_model.value\n",
    "\n",
    "def load_and_preprocess_frames(frame_bytes):\n",
    "    frame = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "    frame = cv2.imdecode(frame, cv2.IMREAD_COLOR)\n",
    "    return frame\n",
    "\n",
    "def predict(frame_bytes):\n",
    "    image = load_and_preprocess_frames(frame_bytes)\n",
    "    prediction = model.predict(image)\n",
    "    ret, buffer = cv2.imencode('.jpg', prediction[0].plot())\n",
    "    print(buffer)\n",
    "    return buffer.tobytes()\n",
    "\n",
    "predict_udf = udf(predict, BinaryType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryWriter(topic_in, topic_out, checkpointPath):\n",
    "    streamRawDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server)  \\\n",
    "    .option(\"subscribe\", topic_in)  \\\n",
    "    .option(\"startingOffsets\",\"latest\").load()\n",
    "\n",
    "    streamRawDF = streamRawDF.withColumn('value1', col('value'))\n",
    "    streamRawDF = streamRawDF.drop('value')\n",
    "    streamRawDF = streamRawDF.withColumn('value', predict_udf('value1'))\n",
    "\n",
    "    query = streamRawDF.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option('topic', topic_out) \\\n",
    "    .option(\"checkpointLocation\", f'../checkpoint/' + checkpointPath) \\\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Streaming Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1 = 'uav1'\n",
    "checkpoint_2 = 'uav2'\n",
    "checkpoint_3 = 'uav3'\n",
    "\n",
    "query_1 = queryWriter(topic_in_1, topic_out_1, checkpoint_1)\n",
    "query_2 = queryWriter(topic_in_2, topic_out_2, checkpoint_2)\n",
    "query_3 = queryWriter(topic_in_3, topic_out_3, checkpoint_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x768cf4d67370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1.start() \n",
    "query_2.start()\n",
    "query_3.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
